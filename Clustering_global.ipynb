{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKPBjo8NTsxBSdxbhiYGll",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veronica-araoz/veronica-araoz_clustering/blob/main/Clustering_global.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1) Carga de datos (Google Colab + Google Drive)\n",
        "# ============================================================\n",
        "# Este notebook fue ejecutado en Google Colab.\n",
        "# Si se trabaja en Colab, se puede montar Google Drive para acceder al dataset.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "dpqabV5dcIO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ruta al dataset (archivo privado en Google Drive; no se incluye en el repositorio)\n",
        "DATASET_PATH = \"/content/drive/MyDrive/CLUSTERING/cluster_global.xlsx\"\n",
        "\n",
        "df = pd.read_excel(DATASET_PATH)\n",
        "\n",
        "print(f\"‚úÖ Archivo cargado correctamente: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
        "print(\"Columnas disponibles:\", df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "W5GDW9kte0ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup y preparaci√≥n del corpus\n",
        "Instalaci√≥n de dependencias (Colab) y creaci√≥n de una columna unificada de texto para el an√°lisis"
      ],
      "metadata": {
        "id": "k6EyLPwmZlzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1) Setup + preparaci√≥n del corpus\n",
        "# ============================================================\n",
        "\n",
        "# Instalaci√≥n de dependencias (solo necesario en Google Colab)\n",
        "!pip -q install transformers sentence_transformers umap-learn\n",
        "!pip -q install --upgrade scikit-learn openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# ---------------------\n",
        "# Recursos NLTK\n",
        "# ---------------------\n",
        "try:\n",
        "    nltk.data.find(\"corpora/stopwords\")\n",
        "except LookupError:\n",
        "    nltk.download(\"stopwords\")\n",
        "\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt\")\n",
        "\n",
        "try:\n",
        "    nltk.data.find(\"corpora/wordnet\")\n",
        "except LookupError:\n",
        "    nltk.download(\"wordnet\")\n",
        "\n",
        "# ---------------------\n",
        "# Carga de datos\n",
        "# ---------------------\n",
        "DATASET_PATH = \"/content/drive/MyDrive/CLUSTERING/cluster_global.xlsx\"  # archivo privado (no incluido en el repo)\n",
        "df = pd.read_excel(DATASET_PATH)\n",
        "\n",
        "# ---------------------\n",
        "# Preparaci√≥n de texto\n",
        "# ---------------------\n",
        "df[\"Titular\"] = df[\"Titular\"].fillna(\"\").astype(str)\n",
        "df[\"Bajada\"] = df[\"Bajada\"].fillna(\"\").astype(str)\n",
        "df[\"Cuerpo_texto\"] = df[\"Cuerpo_texto\"].fillna(\"\").astype(str)\n",
        "\n",
        "df[\"texto_completo\"] = df[[\"Titular\", \"Bajada\", \"Cuerpo_texto\"]].agg(\" \".join, axis=1)\n",
        "\n",
        "print(\"‚úÖ Paso 1 completado: datos cargados y columna 'texto_completo' generada.\")\n",
        "print(\"Columnas disponibles:\", df.columns.tolist())\n",
        "print(\"\\nEjemplo de texto combinado:\\n\", df[\"texto_completo\"].iloc[0][:500])\n"
      ],
      "metadata": {
        "id": "sOG_kluxNXic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.  Embeddings + reducci√≥n de dimensionalidad (UMAP)\n",
        "Se generan embeddings sem√°nticos a nivel documento con SentenceTransformer (modelo multiling√ºe) a partir de la columna texto_completo. Luego se aplica UMAP para reducir dimensionalidad y facilitar el clustering."
      ],
      "metadata": {
        "id": "k39UfEdgfuaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2) Embeddings + reducci√≥n de dimensionalidad (UMAP)\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------\n",
        "# Embeddings (SentenceTransformer)\n",
        "# ---------------------\n",
        "MODEL_NAME = \"distiluse-base-multilingual-cased-v1\"  # modelo multiling√ºe (adecuado para espa√±ol)\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "texts = df[\"texto_completo\"].fillna(\"\").tolist()\n",
        "embeddings = model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "print(\"‚úÖ Embeddings generados.\")\n",
        "print(\"Forma de la matriz de embeddings:\", embeddings.shape)  # (n_notas, 512)\n",
        "\n",
        "# ---------------------\n",
        "# UMAP para reducci√≥n de dimensionalidad\n",
        "# ---------------------\n",
        "# Se reduce a 5 dimensiones para facilitar el clustering manteniendo estructura sem√°ntica.\n",
        "reducer = UMAP(\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    n_components=5,\n",
        "    metric=\"cosine\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "\n",
        "print(\"‚úÖ Dimensionalidad reducida con UMAP.\")\n",
        "print(\"Forma de la matriz reducida:\", reduced_embeddings.shape)  # (n_notas, 5)\n"
      ],
      "metadata": {
        "id": "jtRySsocOREY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.  Selecci√≥n del n√∫mero de cl√∫steres (K)\n",
        "Se eval√∫an distintos valores de K mediante el coeficiente de silueta y el m√©todo del codo (inercia) para fundamentar la elecci√≥n final."
      ],
      "metadata": {
        "id": "uHmyzcOcjppA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3) Selecci√≥n de K (Silueta + Codo)\n",
        "# ============================================================\n",
        "\n",
        "K_MIN = 2\n",
        "K_MAX = 10\n",
        "k_range = range(K_MIN, K_MAX)\n",
        "\n",
        "silhouettes = []\n",
        "inercia = []\n",
        "\n",
        "print(\"\\n--- Evaluaci√≥n de K ---\")\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(reduced_embeddings)\n",
        "\n",
        "    sil_score = silhouette_score(reduced_embeddings, labels)\n",
        "    silhouettes.append(sil_score)\n",
        "\n",
        "    inercia.append(kmeans.inertia_)\n",
        "\n",
        "    print(f\"K={k} | Silueta={sil_score:.4f} | Inercia={kmeans.inertia_:.2f}\")\n",
        "\n",
        "best_k_silhouette = list(k_range)[int(np.argmax(silhouettes))]\n",
        "print(f\"\\n‚úÖ Mejor K seg√∫n Silueta: {best_k_silhouette}\")\n",
        "\n",
        "# ---------------------\n",
        "# Gr√°fico: M√©todo del Codo\n",
        "# ---------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(k_range), inercia, marker=\"o\")\n",
        "plt.title(\"M√©todo del Codo (corpus completo)\")\n",
        "plt.xlabel(\"N√∫mero de cl√∫steres (K)\")\n",
        "plt.ylabel(\"Inercia\")\n",
        "plt.xticks(list(k_range))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ---------------------\n",
        "# Gr√°fico: Silueta\n",
        "# ---------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(k_range), silhouettes, marker=\"o\")\n",
        "plt.title(\"M√©todo de la Silueta (corpus completo)\")\n",
        "plt.xlabel(\"N√∫mero de cl√∫steres (K)\")\n",
        "plt.ylabel(\"Silueta\")\n",
        "plt.xticks(list(k_range))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GHiKtylyOvet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretaci√≥n de la cantidad de cl√∫steres (selecci√≥n de k)\n",
        "\n",
        "Para decidir el n√∫mero √≥ptimo de cl√∫steres (k), se utilizaron dos criterios complementarios: coeficiente de silueta y m√©todo del codo (inercia).\n",
        "\n",
        "1) Coeficiente de silueta\n",
        "\n",
        "El coeficiente de silueta eval√∫a simult√°neamente cohesi√≥n interna (qu√© tan similares son los elementos dentro de un cl√∫ster) y separaci√≥n entre cl√∫steres.\n",
        "\n",
        "Valores cercanos a 1 ‚Üí cl√∫steres bien definidos y separados\n",
        "\n",
        "Valores cercanos a 0 ‚Üí cl√∫steres superpuestos o poco distinguibles\n",
        "\n",
        "Valores negativos ‚Üí asignaci√≥n deficiente (elementos m√°s cercanos a otros cl√∫steres que al propio)\n",
        "\n",
        "‚û°Ô∏è En este caso, la silueta m√°xima fue 0.7556 con k = 2, lo que sugiere que una partici√≥n en dos cl√∫steres produce la estructura m√°s consistente y separada del corpus.\n",
        "\n",
        "2) M√©todo del codo (inercia)\n",
        "\n",
        "La inercia mide la suma de distancias de los puntos al centroide de su cl√∫ster. A medida que aumenta k, la inercia disminuye, pero el objetivo es identificar un punto a partir del cual la mejora se vuelve marginal (el ‚Äúcodo‚Äù).\n",
        "\n",
        "En este an√°lisis:\n",
        "\n",
        "La inercia disminuye de forma continua al aumentar k.\n",
        "\n",
        "La ca√≠da m√°s marcada se observa al pasar de k = 2 a k = 3 (aprox. 907 puntos).\n",
        "\n",
        "A partir de k = 3‚Äì4, la reducci√≥n se vuelve m√°s gradual.\n",
        "\n",
        "‚û°Ô∏è Esto indica que k = 2 o k = 3 pueden representar un buen equilibrio entre simplicidad interpretativa y capacidad explicativa del modelo."
      ],
      "metadata": {
        "id": "zlUpI98dmWxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Clustering final con K-Means\n",
        "Se entrena un modelo K-Means con el valor de k seleccionado y se asigna una etiqueta de cl√∫ster a cada nota del corpus."
      ],
      "metadata": {
        "id": "p-fPzyDoncWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4) Clustering final (K-Means)\n",
        "# ============================================================\n",
        "\n",
        "k_final = 2  # valor seleccionado seg√∫n silueta y m√©todo del codo\n",
        "\n",
        "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)\n",
        "df[\"cluster_label\"] = kmeans_final.fit_predict(reduced_embeddings)\n",
        "\n",
        "print(f\"\\n‚úÖ Conteo de notas por cl√∫ster (K={k_final}):\")\n",
        "print(df[\"cluster_label\"].value_counts().sort_index())\n",
        "\n",
        "# ---------------------\n",
        "# Guardar resultados\n",
        "# ---------------------\n",
        "import os\n",
        "\n",
        "OUTPUT_DIR = \"/content/outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "output_path = os.path.join(OUTPUT_DIR, \"clusters_global.xlsx\")\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Resultados guardados en: {output_path}\")\n"
      ],
      "metadata": {
        "id": "vtGmT2vPPgtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizaciones\n",
        "Gr√°fico de dispersi√≥n en 2D (UMAP)\n",
        "\n",
        "Para facilitar la interpretaci√≥n de los resultados, se realiza una reducci√≥n adicional a 2 dimensiones mediante UMAP y se visualizan las notas period√≠sticas en un plano 2D, coloreadas seg√∫n la etiqueta de cl√∫ster asignada por K-Means. Esta visualizaci√≥n permite explorar la separaci√≥n (o superposici√≥n) entre agrupamientos y detectar patrones generales en el corpus."
      ],
      "metadata": {
        "id": "HWxZIo_ynr9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Visualizaci√≥n en 2D con UMAP (flexible para cualquier n√∫mero de clusters)\n",
        "# =====================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from umap import UMAP\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "clusters_unicos = sorted(df[\"cluster_label\"].unique())\n",
        "n_clusters = len(clusters_unicos)\n",
        "\n",
        "reducer_2d = UMAP(n_components=2, random_state=42, metric=\"cosine\")\n",
        "embeddings_2d = reducer_2d.fit_transform(reduced_embeddings)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=embeddings_2d[:, 0],\n",
        "    y=embeddings_2d[:, 1],\n",
        "    hue=df[\"cluster_label\"],\n",
        "    palette=\"tab10\",\n",
        "    s=50,\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "plt.title(f\"Visualizaci√≥n de Cl√∫steres en 2D - Corpus Global (k={n_clusters})\")\n",
        "plt.xlabel(\"UMAP 1\")\n",
        "plt.ylabel(\"UMAP 2\")\n",
        "plt.legend(title=\"Cluster\", loc=\"best\")\n",
        "plt.tight_layout()\n",
        "\n",
        "output_path_fig = \"/content/clusters_global_2D.png\"\n",
        "plt.savefig(output_path_fig, dpi=300, bbox_inches=\"tight\")\n",
        "print(f\"‚úÖ Gr√°fico guardado en: {output_path_fig}\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WCdCAa3mP9xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretaci√≥n de clusters: Nubes de palabras (WordCloud)\n",
        "\n",
        "Para explorar el contenido tem√°tico de cada cl√∫ster, se generan **nubes de palabras** a partir del texto completo (`Titular + Bajada + Cuerpo_texto`) de las notas asignadas a cada agrupamiento.\n",
        "\n",
        "üìå **Nota metodol√≥gica:** se eliminan stopwords en espa√±ol y un conjunto de t√©rminos irrelevantes frecuentes en textos period√≠sticos, con el fin de resaltar palabras con mayor capacidad descriptiva."
      ],
      "metadata": {
        "id": "G4icZbGuQ8Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Si est√°s en Google Colab y no ten√©s instalada la librer√≠a:\n",
        "# !pip install wordcloud\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# =====================\n",
        "# Recursos NLTK\n",
        "# =====================\n",
        "try:\n",
        "    nltk.data.find(\"corpora/stopwords\")\n",
        "except:\n",
        "    nltk.download(\"stopwords\")\n",
        "\n",
        "# =====================\n",
        "# Stopwords + t√©rminos irrelevantes (ajustables)\n",
        "# =====================\n",
        "palabras_irrelevantes = [\n",
        "    \"noticias\", \"relacionadas\", \"ver\", \"m√°s\", \"adem√°s\", \"as√≠\",\n",
        "    \"comentarios\", \"nan\", \"aunque\", \"solo\", \"uno\", \"aun\"\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words(\"spanish\"))\n",
        "stop_words.update(palabras_irrelevantes)\n",
        "\n",
        "print(\"\\n--- Generando nubes de palabras por cl√∫ster (corpus global) ---\")\n",
        "\n",
        "# =====================\n",
        "# Agrupar textos por cl√∫ster\n",
        "# =====================\n",
        "cluster_texts = df.groupby(\"cluster_label\")[\"texto_completo\"].apply(list).to_dict()\n",
        "\n",
        "for cluster_id, texts in cluster_texts.items():\n",
        "    all_text = \" \".join(texts).lower()\n",
        "\n",
        "    # Preprocesamiento b√°sico\n",
        "    all_text = re.sub(r\"\\d+\", \"\", all_text)  # eliminar n√∫meros\n",
        "    all_text = re.sub(r\"[^\\w\\s]\", \" \", all_text)  # eliminar signos\n",
        "    all_text = re.sub(r\"\\s+\", \" \", all_text).strip()  # limpiar espacios\n",
        "\n",
        "    # Filtrar stopwords y palabras cortas\n",
        "    all_text = \" \".join([\n",
        "        word for word in all_text.split()\n",
        "        if word not in stop_words and len(word) > 2\n",
        "    ])\n",
        "\n",
        "    # WordCloud\n",
        "    wordcloud = WordCloud(\n",
        "        width=900,\n",
        "        height=450,\n",
        "        background_color=\"white\",\n",
        "        collocations=False\n",
        "    ).generate(all_text)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Nube de palabras - Cl√∫ster {cluster_id} (Corpus Global)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "IJ4ZE2K8RM3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretaci√≥n de clusters: palabras clave y palabras distintivas\n",
        "\n",
        "Adem√°s de la visualizaci√≥n, se realiza una exploraci√≥n lexical para caracterizar cada cl√∫ster:\n",
        "\n",
        "- **Palabras clave:** t√©rminos m√°s frecuentes dentro de cada cl√∫ster (Top 10).\n",
        "- **Palabras distintivas:** t√©rminos que aparecen exclusivamente en un cl√∫ster y no en los dem√°s, ordenados por frecuencia.\n",
        "\n",
        "Este paso permite una aproximaci√≥n r√°pida a los ejes tem√°ticos predominantes en cada agrupamiento.\n"
      ],
      "metadata": {
        "id": "Lm4OovnwRZSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from IPython.display import display\n",
        "\n",
        "# =====================\n",
        "# Preparar stopwords\n",
        "# =====================\n",
        "try:\n",
        "    nltk.data.find(\"corpora/stopwords\")\n",
        "except:\n",
        "    nltk.download(\"stopwords\")\n",
        "\n",
        "palabras_irrelevantes = [\n",
        "    \"noticias\", \"relacionadas\", \"ver\", \"m√°s\", \"adem√°s\", \"as√≠\",\n",
        "    \"comentarios\", \"nan\", \"aunque\", \"solo\", \"s√≥lo\", \"ciento\", \"dos\", \"san\",\n",
        "    \"ser\", \"a√±os\", \"a√±o\", \"cada\", \"muchos\", \"uno\", \"aun\"\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words(\"spanish\"))\n",
        "stop_words.update(palabras_irrelevantes)\n",
        "\n",
        "# =====================\n",
        "# Agrupar textos por cluster\n",
        "# =====================\n",
        "cluster_texts = df.groupby(\"cluster_label\")[\"texto_completo\"].apply(list).to_dict()\n",
        "\n",
        "# =====================\n",
        "# Funci√≥n para limpiar y tokenizar\n",
        "# =====================\n",
        "def limpiar_tokenizar(texto: str):\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r\"\\d+\", \"\", texto)  # eliminar n√∫meros\n",
        "    palabras = re.findall(r\"\\b[a-z√°√©√≠√≥√∫√±]{3,}\\b\", texto)  # tokens >= 3 letras\n",
        "    palabras = [w for w in palabras if w not in stop_words]\n",
        "    return palabras\n",
        "\n",
        "# =====================\n",
        "# 1) Palabras clave (Top N m√°s frecuentes)\n",
        "# =====================\n",
        "n_palabras = 10\n",
        "palabras_clave = {}\n",
        "\n",
        "for cluster_id, texts in cluster_texts.items():\n",
        "    all_words = []\n",
        "    for txt in texts:\n",
        "        all_words.extend(limpiar_tokenizar(txt))\n",
        "\n",
        "    freq = Counter(all_words)\n",
        "    palabras_clave[cluster_id] = [w for w, _ in freq.most_common(n_palabras)]\n",
        "\n",
        "df_palabras_clave = pd.DataFrame({k: pd.Series(v) for k, v in palabras_clave.items()})\n",
        "\n",
        "print(\"\\n=== Palabras clave por cluster (Top 10 m√°s frecuentes) ===\")\n",
        "display(df_palabras_clave)\n",
        "\n",
        "# =====================\n",
        "# 2) Palabras distintivas por cluster (exclusivas)\n",
        "# =====================\n",
        "top_n_distintivas = 10\n",
        "palabras_distintivas = {}\n",
        "\n",
        "# Frecuencias por cluster\n",
        "cluster_freqs = {}\n",
        "cluster_sets = {}\n",
        "\n",
        "for cluster_id, texts in cluster_texts.items():\n",
        "    all_words = []\n",
        "    for txt in texts:\n",
        "        all_words.extend(limpiar_tokenizar(txt))\n",
        "\n",
        "    cluster_freqs[cluster_id] = Counter(all_words)\n",
        "    cluster_sets[cluster_id] = set(all_words)\n",
        "\n",
        "for cluster_id, words_set in cluster_sets.items():\n",
        "    otras = set().union(*(s for cid, s in cluster_sets.items() if cid != cluster_id))\n",
        "    exclusivas = words_set - otras\n",
        "\n",
        "    top_exclusivas = [\n",
        "        w for w, _ in cluster_freqs[cluster_id].most_common()\n",
        "        if w in exclusivas\n",
        "    ][:top_n_distintivas]\n",
        "\n",
        "    palabras_distintivas[cluster_id] = top_exclusivas\n",
        "\n",
        "df_palabras_distintivas = pd.DataFrame({k: pd.Series(v) for k, v in palabras_distintivas.items()})\n",
        "\n",
        "print(\"\\n=== Palabras distintivas por cluster (Top 10 exclusivas) ===\")\n",
        "display(df_palabras_distintivas)\n"
      ],
      "metadata": {
        "id": "p_03OIdFRZ2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}