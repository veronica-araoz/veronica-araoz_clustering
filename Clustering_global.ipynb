{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFwDPIQAfiJ8bOMt9lDVI5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veronica-araoz/veronica-araoz_clustering/blob/main/Clustering_global.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este cuaderno la idea es aplicar la técnica de clustering sobre un conjunto de notas periodísticas para identificar agrupaciones temáticas en la totalidad del corpus."
      ],
      "metadata": {
        "id": "aPEizNV6Zkfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero conectamos este notebook con mi Drive para consultar directamente el dataset sin tener que cargarlo manualmente"
      ],
      "metadata": {
        "id": "7n0i2aejcJiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "dpqabV5dcIO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CLUSTERING/cluster_global.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "print(\"Archivo cargado correctamente. Filas y columnas:\", df.shape)\n",
        "print(\"Columnas:\", df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "W5GDW9kte0ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 1: Instalación de Librerías y Carga de Datos\n",
        "Primero, instala las bibliotecas necesarias. Luego, carga tu DataFrame y prepara el texto para el análisis. Este paso combina las columnas de texto en una sola para que los modelos de embeddings puedan procesarlas de manera efectiva."
      ],
      "metadata": {
        "id": "k6EyLPwmZlzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de librerías (solo necesario si aún no las tenés instaladas en Colab)\n",
        "!pip install transformers sentence_transformers umap-learn\n",
        "!pip install --upgrade scikit-learn openpyxl\n",
        "\n",
        "# =====================\n",
        "# Importar librerías\n",
        "# =====================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =====================\n",
        "# Descargar recursos de NLTK\n",
        "# =====================\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "# =====================\n",
        "# Cargar el DataFrame desde Drive\n",
        "# =====================\n",
        "file_path = '/content/drive/MyDrive/CLUSTERING/cluster_global.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# =====================\n",
        "# Preparación y Conversión de Variables\n",
        "# =====================\n",
        "# Mantener ID como está (posiblemente int o str)\n",
        "df['Titular'] = df['Titular'].fillna('').astype(str)\n",
        "df['Bajada'] = df['Bajada'].fillna('').astype(str)\n",
        "df['Cuerpo_texto'] = df['Cuerpo_texto'].fillna('').astype(str)\n",
        "\n",
        "# Concatenar columnas de texto en una sola\n",
        "df['texto_completo'] = df[['Titular', 'Bajada', 'Cuerpo_texto']].agg(' '.join, axis=1)\n",
        "\n",
        "print(\"✅ Paso 1 completado: Librerías instaladas y datos cargados.\")\n",
        "print(\"Columnas de tu DataFrame:\", df.columns.tolist())\n",
        "print(\"\\nEjemplo de texto combinado:\\n\", df['texto_completo'].iloc[0][:500])\n"
      ],
      "metadata": {
        "id": "fxqewRLDgbgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 2: Generación de Embeddings y Reducción de Dimensionalidad (UMAP) Este paso trabaja con los datos del DataFrame completo, generando una única matriz de embeddings para todas las notas. Luego, esta matriz se utilizará para filtrar por medio en el siguiente paso.\n",
        "\n",
        "Sobre SentenceTransformer: Entiende la Semántica: El modelo no solo sabe qué es una palabra, sino que también comprende su relación con otras palabras. Por ejemplo, el modelo ya aprendió que \"marcha\" y \"protesta\" son muy similares en significado, mientras que \"marcha\" y \"computadora\" son muy diferentes. Lo hace sin que tú tengas que enseñárselo.\n",
        "\n",
        "Maneja la Morfología: Al estar pre-entrenado, el modelo ya sabe que palabras como \"marchó,\" \"marchaban\" y \"marcha\" son variaciones de la misma raíz, \"marchar.\" Esta es la razón por la que no necesitas hacer explícitamente la lematización; el modelo ya ha aprendido a manejar estas variaciones."
      ],
      "metadata": {
        "id": "k39UfEdgfuaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Generación de embeddings\n",
        "# =====================\n",
        "\n",
        "# Modelo preentrenado multilingüe (funciona bien en español)\n",
        "model_name = 'distiluse-base-multilingual-cased-v1'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Generar embeddings para cada nota del corpus\n",
        "embeddings = model.encode(df['texto_completo'].tolist(), show_progress_bar=True)\n",
        "\n",
        "print(\"✅ Embeddings generados.\")\n",
        "print(\"Forma de la matriz de embeddings:\", embeddings.shape)  # (n_notas, 512)\n",
        "\n",
        "# =====================\n",
        "# Reducción de dimensionalidad con UMAP\n",
        "# =====================\n",
        "# n_neighbors: equilibrio entre estructura local/global\n",
        "# min_dist: controla dispersión de los puntos\n",
        "# n_components: cuántas dimensiones queremos conservar\n",
        "reducer = UMAP(\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    n_components=5,\n",
        "    metric='cosine',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "\n",
        "print(\"✅ Dimensionalidad reducida con UMAP.\")\n",
        "print(\"Forma de la matriz reducida:\", reduced_embeddings.shape)  # (n_notas, 5)\n"
      ],
      "metadata": {
        "id": "4foPt-b6hXiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3:  Encontrar el k óptimo (Análisis y Decisión)\n",
        "Este código evalúa el rango de clústeres y muestra los resultados de la silueta y el codo."
      ],
      "metadata": {
        "id": "uHmyzcOcjppA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# =====================\n",
        "# Análisis del k óptimo\n",
        "# =====================\n",
        "# Definimos el rango de k a evaluar\n",
        "k_range = range(2, 10)\n",
        "\n",
        "# 1. Método de la Silueta\n",
        "silhouettes = []\n",
        "print(\"\\n--- Método de la Silueta ---\")\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(reduced_embeddings)\n",
        "    score = silhouette_score(reduced_embeddings, labels)\n",
        "    silhouettes.append(score)\n",
        "    print(f\"Silueta para k={k}: {score:.4f}\")\n",
        "\n",
        "# Mejor k según la silueta\n",
        "best_k_silhouette = k_range[np.argmax(silhouettes)]\n",
        "print(f\"\\nNúmero óptimo de clústeres (Silueta): {best_k_silhouette}\")\n",
        "\n",
        "# 2. Método del Codo\n",
        "inercia = []\n",
        "print(\"\\n--- Método del Codo ---\")\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(reduced_embeddings)\n",
        "    inercia.append(kmeans.inertia_)\n",
        "    print(f\"Inercia para k={k}: {kmeans.inertia_:.2f}\")\n",
        "\n",
        "# Gráfico del Método del Codo\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, inercia, marker='o', linestyle='-')\n",
        "plt.title('Método del Codo para todo el dataset')\n",
        "plt.xlabel('Número de Clústeres (K)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Gráfico del Método de la Silueta\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, silhouettes, marker='o', linestyle='-')\n",
        "plt.title('Método de la Silueta para todo el dataset')\n",
        "plt.xlabel('Número de Clústeres (K)')\n",
        "plt.ylabel('Silueta')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qvjv65GeldLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretación sobre la cantidad de clusters:\n",
        "\n",
        "**La silueta** mide qué tan bien separados y cohesionados están los clusters: Valores cercanos a 1 → clusters bien definidos y separados. Valores cercanos a 0 → clusters superpuestos o poco claros. Valores negativos → mala asignación de clusters.\n",
        "\n",
        "--> **La silueta máxima es 0.7556 con k=2**, lo que indica que dividir todo el material en 2 clusters genera la separación más clara y cohesionada.\n",
        "\n",
        "La inercia mide la suma de distancias al centroide dentro de cada cluster: La idea es buscar el “codo”, donde agregar más clusters ya no reduce significativamente la inercia.La inercia baja continuamente al aumentar k, pero el mayor cambio relativo se da al pasar de k=2 a k=3 (una caída de ~907 puntos). Después de k=3-4, la reducción se hace más gradual.\n",
        "Esto indica que **2-3 clusters podrían ser un buen balance** entre simplicidad y explicación del dataset."
      ],
      "metadata": {
        "id": "zlUpI98dmWxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4: Aplicar K-Means con el k elegido (Ejecución)\n",
        "Una vez que has analizado los resultados del bloque anterior, puedes elegir el valor de k_final que más te convenga y ejecutar este bloque."
      ],
      "metadata": {
        "id": "p-fPzyDoncWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# =====================\n",
        "# Definir K final\n",
        "# =====================\n",
        "k_final = 2  # o 3 si querés explorar más detalle\n",
        "\n",
        "# =====================\n",
        "# Aplicar K-Means a todo el corpus\n",
        "# =====================\n",
        "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)\n",
        "df['cluster_label'] = kmeans_final.fit_predict(reduced_embeddings)\n",
        "\n",
        "# Mostrar conteo de notas por cluster\n",
        "print(f\"\\nConteo de notas por clúster con K={k_final}:\")\n",
        "print(df['cluster_label'].value_counts().sort_index())\n",
        "\n",
        "# =====================\n",
        "# Guardar resultados en Excel\n",
        "# =====================\n",
        "output_path = '/content/clusters_global.xlsx'\n",
        "df.to_excel(output_path, index=False)\n",
        "print(f\"✅ Resultados guardados en: {output_path}\")\n"
      ],
      "metadata": {
        "id": "y3OshuVinae8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizaciones:\n",
        "\n",
        "gráfico de dispersión 2D"
      ],
      "metadata": {
        "id": "HWxZIo_ynr9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Visualización en 2D con UMAP (flexible para cualquier número de clusters)\n",
        "# =====================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from umap import UMAP\n",
        "import numpy as np\n",
        "\n",
        "# Configuración de estilo\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Detectar cuántos clusters hay en df['cluster_label']\n",
        "clusters_unicos = sorted(df['cluster_label'].unique())\n",
        "n_clusters = len(clusters_unicos)\n",
        "\n",
        "# Generar colores automáticamente\n",
        "palette = sns.color_palette(\"hsv\", n_clusters)\n",
        "colores_clusters = {cluster: palette[i] for i, cluster in enumerate(clusters_unicos)}\n",
        "\n",
        "# Reducir embeddings a 2D\n",
        "reducer_2d = UMAP(n_components=2, random_state=42, metric='cosine')\n",
        "embeddings_2d = reducer_2d.fit_transform(reduced_embeddings)\n",
        "\n",
        "# Gráfico de dispersión\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=embeddings_2d[:, 0],\n",
        "    y=embeddings_2d[:, 1],\n",
        "    hue=df['cluster_label'],\n",
        "    palette=colores_clusters,\n",
        "    s=50,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title(f'Visualización de Clústeres en 2D - Corpus Global ({n_clusters} clusters)')\n",
        "plt.xlabel('Componente 1')\n",
        "plt.ylabel('Componente 2')\n",
        "plt.legend(title='Clúster', loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Guardar gráfico opcionalmente\n",
        "output_path_fig = '/content/clusters_global_2D.png'\n",
        "plt.savefig(output_path_fig, dpi=300, bbox_inches='tight')\n",
        "print(f\"✅ Gráfico guardado en: {output_path_fig}\")\n"
      ],
      "metadata": {
        "id": "Erz_486CopI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Descargar recursos de NLTK\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Lista de palabras irrelevantes adicionales\n",
        "palabras_irrelevantes = [\n",
        "    'noticias', 'relacionadas', 'ver', 'más', 'además', 'así',\n",
        "    'comentarios', 'nan', 'aunque', 'solo', 'uno', 'aun'\n",
        "]\n",
        "\n",
        "print(\"\\n--- Generando Nubes de Palabras para el corpus global ---\")\n",
        "\n",
        "# Agrupar textos por cluster\n",
        "cluster_texts = defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    cluster_texts[row['cluster_label']].append(row['texto_completo'])\n",
        "\n",
        "# Obtener stopwords en español y añadir palabras irrelevantes\n",
        "stop_words = set(stopwords.words('spanish') + palabras_irrelevantes)\n",
        "\n",
        "for cluster_id, texts in cluster_texts.items():\n",
        "    all_text = ' '.join(texts)\n",
        "\n",
        "    # Preprocesamiento del texto\n",
        "    all_text = all_text.lower()\n",
        "    all_text = re.sub(r'\\d+', '', all_text)\n",
        "    all_text = ' '.join([word for word in all_text.split() if word not in stop_words and len(word) > 2])\n",
        "\n",
        "    # Generar nube de palabras\n",
        "    wordcloud = WordCloud(\n",
        "        width=800,\n",
        "        height=400,\n",
        "        background_color='white'\n",
        "    ).generate(all_text)\n",
        "\n",
        "    # Mostrar gráfico\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Nube de Palabras para el Clúster {cluster_id} - Corpus Global')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "b_oqkVGhqeTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# =====================\n",
        "# Preparar stopwords\n",
        "# =====================\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "palabras_irrelevantes = [\n",
        "    'noticias', 'relacionadas', 'ver', 'más', 'además', 'así',\n",
        "    'comentarios', 'nan', 'aunque', 'solo', 'sólo', 'ciento', 'dos', 'san', 'nan',\n",
        "    'ser', 'años', 'año', 'cada', 'muchos', 'uno', 'aun'\n",
        "]\n",
        "stop_words = set(stopwords.words('spanish') + palabras_irrelevantes)\n",
        "\n",
        "# =====================\n",
        "# Agrupar textos por cluster\n",
        "# =====================\n",
        "cluster_texts = defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    cluster_texts[row['cluster_label']].append(row['texto_completo'])\n",
        "\n",
        "# =====================\n",
        "# Función para limpiar y tokenizar\n",
        "# =====================\n",
        "def limpiar_tokenizar(texto):\n",
        "    # Convertir a minúsculas\n",
        "    texto = texto.lower()\n",
        "    # Eliminar números\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "    # Solo letras (a-z, áéíóúñ) y palabras de al menos 3 letras\n",
        "    palabras = re.findall(r'\\b[a-záéíóúñ]{3,}\\b', texto)\n",
        "    # Eliminar stopwords\n",
        "    palabras = [w for w in palabras if w not in stop_words]\n",
        "    return palabras\n",
        "\n",
        "# =====================\n",
        "# 1️⃣ Tabla de palabras clave (10 más frecuentes)\n",
        "# =====================\n",
        "palabras_clave = {}\n",
        "n_palabras = 10\n",
        "\n",
        "for cluster_id, texts in cluster_texts.items():\n",
        "    all_words = []\n",
        "    for txt in texts:\n",
        "        all_words.extend(limpiar_tokenizar(txt))\n",
        "    freq = Counter(all_words)\n",
        "    palabras_clave[cluster_id] = [w for w, _ in freq.most_common(n_palabras)]\n",
        "\n",
        "df_palabras_clave = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in palabras_clave.items()]))\n",
        "print(\"\\n=== Palabras clave por cluster (10 más frecuentes) ===\")\n",
        "display(df_palabras_clave)\n",
        "\n",
        "# =====================\n",
        "# 2️⃣ Tabla de palabras distintivas por cluster (Top N por frecuencia)\n",
        "# =====================\n",
        "top_n_distintivas = 10  # número de palabras distintivas más relevantes a mostrar\n",
        "palabras_distintivas = {}\n",
        "\n",
        "# Crear sets de palabras limpias por cluster\n",
        "cluster_sets = {}\n",
        "cluster_freqs = {}\n",
        "for cluster_id, texts in cluster_texts.items():\n",
        "    all_words = []\n",
        "    for txt in texts:\n",
        "        all_words.extend(limpiar_tokenizar(txt))\n",
        "    cluster_sets[cluster_id] = set(all_words)\n",
        "    cluster_freqs[cluster_id] = Counter(all_words)\n",
        "\n",
        "# Identificar palabras exclusivas y tomar Top N por frecuencia\n",
        "for cluster_id, words in cluster_sets.items():\n",
        "    otras = set().union(*(s for cid, s in cluster_sets.items() if cid != cluster_id))\n",
        "    exclusivas = words - otras\n",
        "    # Filtrar las palabras exclusivas por frecuencia y tomar top N\n",
        "    top_exclusivas = [w for w, _ in cluster_freqs[cluster_id].most_common() if w in exclusivas][:top_n_distintivas]\n",
        "    palabras_distintivas[cluster_id] = top_exclusivas\n",
        "\n",
        "df_palabras_distintivas = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in palabras_distintivas.items()]))\n",
        "print(\"\\n=== Palabras distintivas por cluster (Top N más relevantes) ===\")\n",
        "display(df_palabras_distintivas)\n"
      ],
      "metadata": {
        "id": "7aU5Oibnzu5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}