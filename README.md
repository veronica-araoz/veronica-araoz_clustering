Clustering de notas period√≠sticas con embeddings + UMAP + K-Means

Este repositorio contiene el c√≥digo desarrollado en el marco de mi tesis doctoral y beca doctoral de CONICET, con el objetivo de realizar una aproximaci√≥n exploratoria al corpus mediante t√©cnicas de clustering no supervisado aplicadas a textos period√≠sticos. El prop√≥sito principal es identificar agrupamientos tem√°ticos/discursivos y explorar permanencias o variaciones en el conjunto de datos.

El corpus est√° compuesto por notas publicadas en tres portales de noticias online de Argentina (La Voz de C√≥rdoba, Diario Uno de Mendoza y La Capital de Rosario) durante el per√≠odo 2015‚Äì2020, recolectadas por su vinculaci√≥n con las movilizaciones feministas. Esto responde al objetivo principal de la investigaci√≥n: comprender la producci√≥n discursiva en torno a movilizaciones feministas por parte de la prensa de alcance local y regional en Argentina durante 2015‚Äì2020.

Objetivos del notebook

El pipeline implementado permite:

Cargar y preparar un dataset de notas period√≠sticas con campos textuales.

Unificar texto (Titular + Bajada + Cuerpo_texto) en una √∫nica variable para an√°lisis.

Generar embeddings sem√°nticos mediante modelos preentrenados (SentenceTransformers).

Reducir dimensionalidad con UMAP.

Estimar un k √≥ptimo para clustering (m√©todo del codo + silueta).

Aplicar K-Means para asignar cl√∫steres al corpus.

Explorar resultados con:

visualizaci√≥n 2D (UMAP)

nubes de palabras por cl√∫ster

palabras clave y palabras distintivas por cl√∫ster

Dataset (estructura esperada)

El dataset utilizado no se incluye en este repositorio. Para ejecutar el notebook, se espera un archivo tabular con las siguientes columnas:

ID

Titular

Bajada

Cuerpo_texto

medio (opcional, si se desea segmentar por portal)

anio (opcional, si se desea segmentar por per√≠odo)

En el preprocesamiento, las columnas Titular, Bajada y Cuerpo_texto se concatenan en una variable adicional:

texto_completo

Metodolog√≠a (resumen t√©cnico)
1) Embeddings (SentenceTransformers)

Se utiliza un modelo preentrenado multiling√ºe para representar sem√°nticamente cada nota period√≠stica:

distiluse-base-multilingual-cased-v1

El objetivo es obtener una matriz de embeddings que capture similitudes sem√°nticas entre textos.

2) Reducci√≥n de dimensionalidad (UMAP)

Dado que los embeddings tienen alta dimensionalidad, se aplica UMAP para:

reducir el espacio (por ejemplo a 5 dimensiones) y facilitar el clustering

generar una reducci√≥n adicional a 2 dimensiones para visualizaci√≥n

3) Clustering (K-Means)

Se aplica K-Means sobre los embeddings reducidos.

Para seleccionar el n√∫mero de cl√∫steres k, se utilizan dos criterios complementarios:

M√©todo de la silueta (cohesi√≥n/separaci√≥n entre cl√∫steres)

M√©todo del codo (inercia y punto de inflexi√≥n)

Nota metodol√≥gica sobre la interpretaci√≥n de k

La silueta mide qu√© tan bien separados y cohesionados est√°n los cl√∫steres:

Valores cercanos a 1 ‚Üí cl√∫steres bien definidos y separados

Valores cercanos a 0 ‚Üí cl√∫steres superpuestos o poco claros

Valores negativos ‚Üí mala asignaci√≥n de cl√∫steres

La inercia mide la suma de distancias al centroide dentro de cada cl√∫ster. Se busca el ‚Äúcodo‚Äù, donde agregar m√°s cl√∫steres ya no reduce significativamente la inercia.

En este caso, los resultados sugieren que 2‚Äì3 cl√∫steres podr√≠an ofrecer un balance razonable entre simplicidad e interpretabilidad, aunque la decisi√≥n final depende del objetivo anal√≠tico.

Outputs generados

El notebook produce:

una columna cluster_label con la asignaci√≥n de cl√∫ster por nota

visualizaci√≥n UMAP en 2D del corpus coloreada por cl√∫ster

nubes de palabras por cl√∫ster

tablas de:

palabras clave por cl√∫ster (Top N m√°s frecuentes)

palabras distintivas por cl√∫ster (t√©rminos exclusivos por agrupamiento)

exportaci√≥n opcional del dataset con resultados (por ejemplo a Excel)

Ejecuci√≥n en Google Colab

El notebook fue dise√±ado para ejecutarse en Google Colab.
En caso de utilizar Google Drive, se incluye la celda:

from google.colab import drive
drive.mount('/content/drive')


La ruta del archivo debe definirse en el bloque de carga del dataset (ejemplo):

DATASET_PATH = "/content/drive/MyDrive/CLUSTERING/cluster_global.xlsx"
df = pd.read_excel(DATASET_PATH)

Referencias

Este pipeline fue desarrollado tomando como referencia:

Raimondo Anselmino, Natalia; Rostagno, Jos√©; Cardoso, Ana Laura (2021). Aplicaci√≥n de t√©cnicas de clustering para el estudio sociosemi√≥tico sobre g√©neros period√≠sticos en fanpages de Clar√≠n y La Naci√≥n.

Alomar, Francisco J. (2025). pasantia_TUIA_CIM_Francisco_J._Alomar [Repositorio de c√≥digo]. GitHub:
https://github.com/Fran251184/pasantia_TUIA_CIM_Francisco_J._Alomar

Recolecci√≥n de datos (scraping)

Las notas period√≠sticas que componen el dataset fueron recolectadas mediante t√©cnicas de web scraping, utilizando como referencia herramientas de acceso libre desarrolladas para la extracci√≥n automatizada de art√≠culos de prensa en Argentina:

Basel, V. (2024). Webscraping diarios arg (Version 0.1) [Publicaci√≥n en GitLab].
https://gitlab.com/valentinbasel/webscraping-diarios-arg

Si quer√©s, tambi√©n te armo una versi√≥n m√°s corta para el ‚ÄúAbout‚Äù del repo (una o dos l√≠neas) y te dejo un requirements.txt listo para subir.

quise copiar y pegar todo esto que me pasaste pero no esta adaptado al lenguaje de github entonces no me aparece el formato con titulos, con punteo, etc

Perfecto, eso pasa porque GitHub formatea con Markdown, y si lo peg√°s sin respetar algunos detalles (saltos de l√≠nea, espacios, etc.) queda como texto plano.

Ac√° te lo dejo 100% en Markdown para GitHub, listo para copiar y pegar tal cual en README.md üëá

Clustering de notas period√≠sticas con embeddings + UMAP + K-Means

Este repositorio contiene el c√≥digo desarrollado en el marco de mi tesis doctoral y beca doctoral de CONICET, con el objetivo de realizar una aproximaci√≥n exploratoria al corpus mediante t√©cnicas de clustering no supervisado aplicadas a textos period√≠sticos. El prop√≥sito principal es identificar agrupamientos tem√°ticos/discursivos y explorar permanencias o variaciones en el conjunto de datos.

El corpus est√° compuesto por notas publicadas en tres portales de noticias online de Argentina (La Voz de C√≥rdoba, Diario Uno de Mendoza y La Capital de Rosario) durante el per√≠odo 2015‚Äì2020, recolectadas por su vinculaci√≥n con las movilizaciones feministas. Esto responde al objetivo principal de la investigaci√≥n: comprender la producci√≥n discursiva en torno a movilizaciones feministas por parte de la prensa de alcance local y regional en Argentina durante 2015‚Äì2020.

Objetivos del notebook

El pipeline implementado permite:

Cargar y preparar un dataset de notas period√≠sticas con campos textuales.

Unificar texto (Titular + Bajada + Cuerpo_texto) en una √∫nica variable para an√°lisis.

Generar embeddings sem√°nticos mediante modelos preentrenados (SentenceTransformers).

Reducir dimensionalidad con UMAP.

Estimar un k √≥ptimo para clustering (m√©todo del codo + silueta).

Aplicar K-Means para asignar cl√∫steres al corpus.

Explorar resultados con:

visualizaci√≥n 2D (UMAP)

nubes de palabras por cl√∫ster

palabras clave y palabras distintivas por cl√∫ster

Dataset (estructura esperada)

El dataset utilizado no se incluye en este repositorio. Para ejecutar el notebook, se espera un archivo tabular con las siguientes columnas:

ID

Titular

Bajada

Cuerpo_texto

medio (opcional, si se desea segmentar por portal)

anio (opcional, si se desea segmentar por per√≠odo)

En el preprocesamiento, las columnas Titular, Bajada y Cuerpo_texto se concatenan en una variable adicional:

texto_completo

Metodolog√≠a (resumen t√©cnico)
1) Embeddings (SentenceTransformers)

Se utiliza un modelo preentrenado multiling√ºe para representar sem√°nticamente cada nota period√≠stica:

distiluse-base-multilingual-cased-v1

El objetivo es obtener una matriz de embeddings que capture similitudes sem√°nticas entre textos.

2) Reducci√≥n de dimensionalidad (UMAP)

Dado que los embeddings tienen alta dimensionalidad, se aplica UMAP para:

reducir el espacio (por ejemplo a 5 dimensiones) y facilitar el clustering

generar una reducci√≥n adicional a 2 dimensiones para visualizaci√≥n

3) Clustering (K-Means)

Se aplica K-Means sobre los embeddings reducidos.

Para seleccionar el n√∫mero de cl√∫steres k, se utilizan dos criterios complementarios:

M√©todo de la silueta (cohesi√≥n/separaci√≥n entre cl√∫steres)

M√©todo del codo (inercia y punto de inflexi√≥n)

Nota metodol√≥gica sobre la interpretaci√≥n de k

La silueta mide qu√© tan bien separados y cohesionados est√°n los cl√∫steres:

valores cercanos a 1 ‚Üí cl√∫steres bien definidos y separados

valores cercanos a 0 ‚Üí cl√∫steres superpuestos o poco claros

valores negativos ‚Üí mala asignaci√≥n de cl√∫steres

La inercia mide la suma de distancias al centroide dentro de cada cl√∫ster. Se busca el ‚Äúcodo‚Äù, donde agregar m√°s cl√∫steres ya no reduce significativamente la inercia.

En este caso, los resultados sugieren que 2‚Äì3 cl√∫steres podr√≠an ofrecer un balance razonable entre simplicidad e interpretabilidad, aunque la decisi√≥n final depende del objetivo anal√≠tico.

Outputs generados

El notebook produce:

una columna cluster_label con la asignaci√≥n de cl√∫ster por nota

visualizaci√≥n UMAP en 2D del corpus coloreada por cl√∫ster

nubes de palabras por cl√∫ster

tablas de:

palabras clave por cl√∫ster (Top N m√°s frecuentes)

palabras distintivas por cl√∫ster (t√©rminos exclusivos por agrupamiento)

exportaci√≥n opcional del dataset con resultados (por ejemplo a Excel)

Ejecuci√≥n en Google Colab

El notebook fue dise√±ado para ejecutarse en Google Colab.

En caso de utilizar Google Drive, se incluye la celda:

from google.colab import drive
drive.mount('/content/drive')


La ruta del archivo debe definirse en el bloque de carga del dataset (ejemplo):

DATASET_PATH = "/content/drive/MyDrive/CLUSTERING/cluster_global.xlsx"
df = pd.read_excel(DATASET_PATH)

Referencias

Este pipeline fue desarrollado tomando como referencia:

Raimondo Anselmino, Natalia; Rostagno, Jos√©; Cardoso, Ana Laura (2021). Aplicaci√≥n de t√©cnicas de clustering para el estudio sociosemi√≥tico sobre g√©neros period√≠sticos en fanpages de Clar√≠n y La Naci√≥n.

Alomar, Francisco J. (2025). pasantia_TUIA_CIM_Francisco_J._Alomar [Repositorio de c√≥digo]. GitHub:
https://github.com/Fran251184/pasantia_TUIA_CIM_Francisco_J._Alomar

Recolecci√≥n de datos (scraping)

Las notas period√≠sticas que componen el dataset fueron recolectadas mediante t√©cnicas de web scraping, utilizando como referencia herramientas de acceso libre desarrolladas para la extracci√≥n automatizada de art√≠culos de prensa en Argentina:

Basel, V. (2024). Webscraping diarios arg (Version 0.1) [Publicaci√≥n en GitLab].
https://gitlab.com/valentinbasel/webscraping-diarios-arg
